# ğŸ›¡ï¸ Toxic Comment Detection using Fine-Tuned DistilBERT  
### Predict Toxic / Non-Toxic comments using NLP + Deep Learning

This project builds an **end-to-end toxic comment detection system** using:

- âš¡ **Baseline Logistic Regression (TF-IDF)**
- ğŸ¤– **Fine-Tuned DistilBERT Transformer**
- ğŸ§¹ Heavy text preprocessing
- ğŸ“Š Performance evaluation & comparison
- ğŸŒ Deployed **Streamlit Web App**
- â˜ï¸ Model hosted on **Hugging Face Hub**

The final model is a **fine-tuned DistilBERT** capable of identifying toxic comments such as:
- Hate speech  
- Harassment  
- Profanity  
- Abusive language  
- Threats  
- Insults  

---

# ğŸš€ Live Demo (Streamlit App)

ğŸ”— **Live App:** *[Add your streamlit URL here]*  
Paste any comment and instantly see toxicity + confidence!

---

# ğŸ¤— Hugging Face Model  
Your fine-tuned model is publicly available:

ğŸ”— **https://huggingface.co/stevehugss/toxic-comment-bert**

This allows anyone to:
- Download the model  
- Use it for inference  
- Integrate into applications  

---

# ğŸ¯ Project Aim

The goal was to build a **robust, real-world toxicity detection system** that:

âœ” Understands context  
âœ” Detects subtle insults  
âœ” Handles sarcasm  
âœ” Performs well on social media-style text  
âœ” Provides easy-to-use inference via a web UI  

Baseline Machine Learning models are fast but fail on contextual toxicity.  
BERT-based transformers capture deep semantic meaning â†’ much higher accuracy.

---

# ğŸ“¦ Dataset  
Dataset used: **Civil Comments Toxicity Dataset (HuggingFace: `civil_comments`)**

- ~900K real comments  
- Contains a continuous toxicity score (0â€“1)  
- Converted to binary:  
  - **1 = Toxic** (score > 0.5)  
  - **0 = Non-Toxic**  
- Balanced sampled dataset for training  
  - 10,000 toxic  
  - 10,000 non-toxic  

---

# ğŸ§¹ Text Preprocessing Pipeline

The text cleaning used for Logistic Regression & BERT fine-tuning:

- Lowercasing  
- Remove URLs  
- Remove punctuation  
- Expand contractions (e.g., â€œcan'tâ€ â†’ â€œcannotâ€)  
- Remove numbers  
- Reduce repeated characters (â€œnoooooâ€ â†’ â€œnooâ€)  
- Tokenization  
- Stopword removal  
- Lemmatization  
- Merge cleaned tokens back into `clean_text`

This ensures the model receives clean, normalized text.

---

# ğŸ§  Model Architecture

## 1ï¸âƒ£ **Baseline: Logistic Regression**
- Vectorizer: **TF-IDF (1-gram & 2-gram)**
- Resampling: RandomOversampler
- Limitations:
  - Cannot understand context  
  - Fails on subtle insults  
  - No semantic understanding  

---

## 2ï¸âƒ£ **Fine-Tuned DistilBERT**
- Base model: `distilbert-base-uncased`
- Added dropout (0.3)
- Gradient checkpointing (memory optimized)
- Mixed-precision training (FP16)
- Max length: 64 tokens (optimized)
- Optimizer: AdamW
- Scheduler: Cosine learning rate decay
- Early Stopping: patience=2
- 20,000 samples (balanced)

---

# ğŸ“Š Results

## â­ Logistic Regression (Baseline)
